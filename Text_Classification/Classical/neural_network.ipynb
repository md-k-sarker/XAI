{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use natural language toolkit\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels(root_data_folder, saved_file):\n",
    "    \"\"\"\n",
    "    Loads 20news group dataset data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # If file is saved then just load the file\n",
    "    if os.path.isfile(saved_file):\n",
    "        x_text, y, y_label = load_data(saved_file)\n",
    "        return [x_text, y, y_label]\n",
    "\n",
    "    else:\n",
    "        # Load data from files\n",
    "        x_text = []\n",
    "        y_label = []\n",
    "        y_textual_label = []\n",
    "        counter = 0\n",
    "        for folder_name in os.listdir(root_data_folder):\n",
    "            if not folder_name.startswith('.'):\n",
    "                for file_name in os.listdir(os.path.join(root_data_folder, folder_name)):\n",
    "\n",
    "                    examples = open(os.path.join(root_data_folder, folder_name, file_name),\n",
    "                                    mode='r', encoding='utf-8', errors='ignore').read().strip()\n",
    "\n",
    "                    # Split by words\n",
    "                    x_text.append(clean_str(examples))\n",
    "                    label = [0] * 20\n",
    "                    label[counter] = 1\n",
    "                    y_label.append(label)\n",
    "                    y_textual_label.append(folder_name)\n",
    "                counter += 1\n",
    "\n",
    "        y = np.concatenate([y_label], 0)\n",
    "        save_data([x_text, y, y_textual_label], saved_file)\n",
    "        return [x_text, y, y_textual_label]\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    with open(os.path.abspath(file_name), 'rb') as f:\n",
    "        x_text, y, y_label = pickle.load(f)\n",
    "        return [x_text, y, y_label]\n",
    "\n",
    "\n",
    "def save_data(data, file_name):\n",
    "    with open(os.path.abspath(file_name), 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18828  documents\n",
      "20  classes ['rec.sport.baseball', 'sci.crypt', 'comp.os.ms-windows.misc', 'rec.autos', 'talk.politics.mideast', 'misc.forsale', 'sci.electronics', 'talk.politics.misc', 'comp.sys.ibm.pc.hardware', 'soc.religion.christian', 'comp.graphics', 'talk.politics.guns', 'sci.space', 'comp.windows.x', 'rec.sport.hockey', 'sci.med', 'comp.sys.mac.hardware', 'talk.religion.misc', 'alt.atheism', 'rec.motorcycles']\n",
      "137817  unique stemmed words\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "root_data_folder = '../../data/20news-18828'\n",
    "saving_data_file = '../../data/preloaded/20news_18828.dt'\n",
    "saving_words_data_file = '../../data/preloaded/20news_all_words.dt'\n",
    "x_text, y, y_label = load_data_and_labels(root_data_folder, saving_data_file)\n",
    "\n",
    "words = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "# If file is saved then just load the file\n",
    "if os.path.isfile(saving_words_data_file):\n",
    "    with open(os.path.abspath(saving_words_data_file), 'rb') as f:\n",
    "        words = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    # loop through each documents in our training data\n",
    "    for text, y_, label in zip(x_text, y, y_label):\n",
    "        # tokenize each word in the document\n",
    "        w = nltk.word_tokenize(text)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "\n",
    "    # stem and lower each word and remove duplicates\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "    words = list(set(words))\n",
    "    with open(os.path.abspath(saving_words_data_file), 'wb') as f:\n",
    "        pickle.dump(words, f)\n",
    "\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "# loop through each documents in our training data\n",
    "for text, label in zip(x_text, y_label):\n",
    "\n",
    "    # add to documents in our corpus\n",
    "    documents.append((text, label))\n",
    "    # add to our classes list\n",
    "    if label not in classes:\n",
    "        classes.append(label)\n",
    "\n",
    "\n",
    "# remove duplicates\n",
    "classes = list(set(classes))\n",
    "\n",
    "#\n",
    "print(len(documents), \" documents\")\n",
    "print(len(classes), \" classes\", classes)\n",
    "print(len(words), \" unique stemmed words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each document\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    training.append(bag)\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    output.append(output_row)\n",
    "\n",
    "print (\"# words\", len(words))\n",
    "print (\"# classes\", len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'ar', 'you', '?']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# sample training/output\n",
    "i = 0\n",
    "w = documents[i][0]\n",
    "print ([stemmer.stem(word.lower()) for word in w])\n",
    "print (training[i])\n",
    "print (output[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    " \n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "def think(sentence, show_details=False):\n",
    "    x = bow(sentence.lower(), words, show_details)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our bag of words\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = sigmoid(np.dot(l0, synapse_0))\n",
    "    # output layer\n",
    "    l2 = sigmoid(np.dot(l1, synapse_1))\n",
    "    return l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANN and Gradient Descent code from https://iamtrask.github.io//2015/07/27/python-network-part2/\n",
    "def train(X, y, hidden_neurons=10, alpha=1, epochs=50000, dropout=False, dropout_percent=0.5):\n",
    "\n",
    "    print (\"Training with %s neurons, alpha:%s, dropout:%s %s\" % (hidden_neurons, str(alpha), dropout, dropout_percent if dropout else '') )\n",
    "    print (\"Input matrix: %sx%s    Output matrix: %sx%s\" % (len(X),len(X[0]),1, len(classes)) )\n",
    "    np.random.seed(1)\n",
    "\n",
    "    last_mean_error = 1\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((len(X[0]), hidden_neurons)) - 1\n",
    "    synapse_1 = 2*np.random.random((hidden_neurons, len(classes))) - 1\n",
    "\n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "\n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "        \n",
    "    for j in iter(range(epochs+1)):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
    "                \n",
    "        if(dropout):\n",
    "            layer_1 *= np.random.binomial([np.ones((len(X),hidden_neurons))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = y - layer_2\n",
    "\n",
    "        if (j% 10000) == 0 and j > 5000:\n",
    "            # if this 10k iteration's error is greater than the last iteration, break out\n",
    "            if np.mean(np.abs(layer_2_error)) < last_mean_error:\n",
    "                print (\"delta after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))) )\n",
    "                last_mean_error = np.mean(np.abs(layer_2_error))\n",
    "            else:\n",
    "                print (\"break:\", np.mean(np.abs(layer_2_error)), \">\", last_mean_error )\n",
    "                break\n",
    "                \n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "        if(j > 0):\n",
    "            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))        \n",
    "        \n",
    "        synapse_1 += alpha * synapse_1_weight_update\n",
    "        synapse_0 += alpha * synapse_0_weight_update\n",
    "        \n",
    "        prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "        prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # persist synapses\n",
    "    synapse = {'synapse0': synapse_0.tolist(), 'synapse1': synapse_1.tolist(),\n",
    "               'datetime': now.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    synapse_file = \"synapses.json\"\n",
    "\n",
    "    with open(synapse_file, 'w') as outfile:\n",
    "        json.dump(synapse, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", synapse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 20 neurons, alpha:0.1, dropout:False \n",
      "Input matrix: 12x26    Output matrix: 1x3\n",
      "delta after 10000 iterations:0.00666786086319\n",
      "delta after 20000 iterations:0.0045541143638\n",
      "delta after 30000 iterations:0.00365376837861\n",
      "delta after 40000 iterations:0.00312820414223\n",
      "delta after 50000 iterations:0.00277455420548\n",
      "delta after 60000 iterations:0.00251621748853\n",
      "delta after 70000 iterations:0.00231708959218\n",
      "delta after 80000 iterations:0.00215766231098\n",
      "delta after 90000 iterations:0.00202636101423\n",
      "delta after 100000 iterations:0.00191583201544\n",
      "saved synapses to: synapses.json\n",
      "processing time: 6.587924957275391 seconds\n"
     ]
    }
   ],
   "source": [
    "X = np.array(training)\n",
    "y = np.array(output)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train(X, y, hidden_neurons=20, alpha=0.1, epochs=100000, dropout=False, dropout_percent=0.2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"processing time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0.99819938892600879]]\n",
      "[[1, 0.99819938892600879]]\n",
      "sudo make me a sandwich \n",
      " classification: [['sandwich', 0.99819938892600879]]\n",
      "[[0, 0.99814068207517803]]\n",
      "[[0, 0.99814068207517803]]\n",
      "how are you today? \n",
      " classification: [['greeting', 0.99814068207517803]]\n",
      "[[2, 0.98867670802444763]]\n",
      "[[2, 0.98867670802444763]]\n",
      "talk to you tomorrow \n",
      " classification: [['goodbye', 0.98867670802444763]]\n",
      "[[0, 0.89982215671891475]]\n",
      "[[0, 0.89982215671891475]]\n",
      "who are you? \n",
      " classification: [['greeting', 0.89982215671891475]]\n",
      "[[1, 0.97520886241661375]]\n",
      "[[1, 0.97520886241661375]]\n",
      "make me some lunch \n",
      " classification: [['sandwich', 0.97520886241661375]]\n",
      "\n",
      "found in bag: how\n",
      "found in bag: yo\n",
      "found in bag: lunch\n",
      "sentence: how was your lunch? \n",
      " bow: [0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[0, 0.92994974842438971], [1, 0.26270575871767921]]\n",
      "[[0, 0.92994974842438971], [1, 0.26270575871767921]]\n",
      "how was your lunch? \n",
      " classification: [['greeting', 0.92994974842438971], ['sandwich', 0.26270575871767921]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['greeting', 0.92994974842438971], ['sandwich', 0.26270575871767921]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.2\n",
    "# load our calculated synapse values\n",
    "synapse_file = 'synapses.json' \n",
    "with open(synapse_file) as data_file: \n",
    "    synapse = json.load(data_file) \n",
    "    synapse_0 = np.asarray(synapse['synapse0']) \n",
    "    synapse_1 = np.asarray(synapse['synapse1'])\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = think(sentence, show_details)\n",
    "\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    print (\"%s \\n classification: %s\" % (sentence, return_results))\n",
    "    return return_results\n",
    "\n",
    "classify(\"sudo make me a sandwich\")\n",
    "classify(\"how are you today?\")\n",
    "classify(\"talk to you tomorrow\")\n",
    "classify(\"who are you?\")\n",
    "classify(\"make me some lunch\")\n",
    "print ()\n",
    "classify(\"how was your lunch?\", show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
